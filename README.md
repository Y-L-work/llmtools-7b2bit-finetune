# llmtools-7b2bit-finetune

ğŸš€ **llmtools-7b2bit-finetune** æ˜¯ä¸€å€‹ä½¿ç”¨ **LLMTools** åœ¨ **RTX 3050 Ti (4GB VRAM)** ä¸Š **å¾®èª¿ LLaMA-7B (2-bit é‡åŒ–)** çš„å°ˆæ¡ˆã€‚

## ğŸ“Œ å°ˆæ¡ˆæ¦‚è¿°
æœ¬å°ˆæ¡ˆçš„ç›®æ¨™æ˜¯é€é **LLMTools** é€²è¡Œ **2-bit LoRA å¾®èª¿**ï¼Œä»¥ä¾¿åœ¨ **è³‡æºå—é™çš„ GPU (å¦‚ RTX 3050 Ti)** ä¸Šé«˜æ•ˆè¨“ç·´å¤§èªè¨€æ¨¡å‹ (**LLaMA-7B**)ã€‚é€™æ¨£çš„å¾®èª¿æ–¹å¼èƒ½å¤ é¡¯è‘—é™ä½ VRAM éœ€æ±‚ï¼Œä½¿ **ä½éšé¡¯å¡ä¹Ÿèƒ½åƒèˆ‡ LLM å¾®èª¿**ã€‚

## ğŸ”§ æŠ€è¡“æ£§
- **LLMTools** - ç”¨æ–¼ LLM ä½æ¯”ç‰¹é‡åŒ–èˆ‡å¾®èª¿
- **PyTorch 2.1.1ï¼ˆCUDA 12.1ï¼‰** - æ·±åº¦å­¸ç¿’æ¡†æ¶
- **Hugging Face Transformers** - æ¨¡å‹ä¸‹è¼‰èˆ‡ç®¡ç†
- **Docker** - å®¹å™¨åŒ–åŸ·è¡Œç’°å¢ƒ
- **Alpaca Dataset** - å¾®èª¿æ™‚ä½¿ç”¨çš„æ¨™æº–æ•¸æ“šé›†

## ğŸ“ å°ˆæ¡ˆç›®éŒ„çµæ§‹
```
llmtools_7b2bit_finetune/
â”‚â”€â”€ src/
â”‚   â”œâ”€â”€ train.py             # ä¸»è¦å¾®èª¿è…³æœ¬
â”‚   â”œâ”€â”€ download_model.py    # ä¸‹è¼‰ 2-bit é‡åŒ–æ¨¡å‹
â”‚   â”œâ”€â”€ preprocess_data.py   # æ•¸æ“šé è™•ç†
â”‚   â”œâ”€â”€ config.py            # è¨­å®šå¾®èª¿åƒæ•¸
â”‚â”€â”€ data/
â”‚   â”œâ”€â”€ alpaca.json          # è¨“ç·´æ•¸æ“š
â”‚â”€â”€ output/                  # ä¿å­˜å¾®èª¿å¾Œçš„æ¨¡å‹
â”‚â”€â”€ requirements.txt         # ä¾è³´å®‰è£
â”‚â”€â”€ run.sh                   # å¿«é€Ÿé‹è¡Œè…³æœ¬
â”‚â”€â”€ README.md                # æœ¬æ–‡ä»¶
â”‚â”€â”€ Dockerfile               # Docker ç’°å¢ƒå®šç¾©
â”‚â”€â”€ docker-compose.yml       # Docker Compose è¨­å®š
```

## ğŸ”¨ å®‰è£èˆ‡ç’°å¢ƒè¨­å®š
### 1ï¸âƒ£ å»ºç«‹ä¸¦å•Ÿå‹• Python ç’°å¢ƒ
```bash
conda create -n llmtools_env python=3.10 -y
conda activate llmtools_env
```

### 2ï¸âƒ£ å®‰è£ä¾è³´
```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ ä¸‹è¼‰ LLaMA-7B 2-bit é‡åŒ–æ¨¡å‹
```bash
python src/download_model.py
```
> **æ³¨æ„**ï¼šé è¨­å­˜æ”¾ä½ç½®ç‚º `D:/huggingface_models/Llama_1_7b_E8P_2Bit/`

## ğŸš€ é‹è¡Œå¾®èª¿
### æ–¹å¼ 1ï¼šç›´æ¥åŸ·è¡Œ Python è…³æœ¬
```bash
python src/train.py
```

### æ–¹å¼ 2ï¼šä½¿ç”¨ Docker é‹è¡Œ
```bash
docker-compose up --build
```

## ğŸ“Œ ç›®å‰å°ˆæ¡ˆé€²åº¦
âœ… **ç’°å¢ƒèˆ‡ä¾è³´å®‰è£**
âœ… **ä¸‹è¼‰ LLaMA-7B 2-bit é‡åŒ–æ¨¡å‹**
âœ… **æ•¸æ“šé è™•ç† (Alpaca Dataset)**
âœ… **åŸºæœ¬å¾®èª¿è…³æœ¬ (`train.py`) å»ºç«‹**
â³ **æ¸¬è©¦ 2-bit å¾®èª¿éç¨‹**
â³ **æ¨¡å‹è¨“ç·´èˆ‡æ•ˆæœé©—è­‰**
â³ **å¾®èª¿å¾Œæ¨¡å‹çš„æ¨ç†æ¸¬è©¦**

**ğŸ“Œ è¨ˆç•«ä¸­çš„æ”¹é€²æ–¹å‘ï¼š**
- **å˜—è©¦ä¸åŒçš„ LoRA è¶…åƒæ•¸èª¿æ•´**
- **æ¢ç´¢æœ€ä½³ 2-bit é‡åŒ–ç­–ç•¥ä»¥é™ä½ VRAM ä½¿ç”¨é‡**
- **å¢åŠ  TensorRT åŠ é€Ÿæ¨ç†**

---

> **ä½œè€…**: @coffeetea
> **GitHub Repo**: [å³å°‡å»ºç«‹]

